---
driver:
  name: docker

platforms:
  # Two groups are defined: vault and vault_all. The vault_all group contains
  # all of the nodes in vault along with an extra pair of nodes.
  
  - &instance_config
    name: vault-1
    groups:
      - vault
      - vault_all
    image: docker.io/geerlingguy/docker-ubuntu2204-ansible
    pre_build_image: true
    override_command: false
    
    # Pass-through any proxy config
    env:
      http_proxy: "$http_proxy"
      https_proxy: "$https_proxy"
      no_proxy: "vault-1,vault-2,vault-3,vault-4,vault-5,vault-6,vault-7,$no_proxy"
    
    # Internal network used for cluster inter-node communication
    networks:
      - name: internal
    
    # Required to run systemd within docker
    privileged: true
    cgroupns_mode: host
    volumes:
      - "/sys/fs/cgroup:/sys/fs/cgroup:rw"
  
  - <<: *instance_config
    name: vault-2
  
  - <<: *instance_config
    name: vault-3
  
  # The following additional 'vault-extra' nodes are only used in a subset of
  # tests involving adding/removing nodes from the cluster.
  #
  # We choose an enlarged cluster size of 7 to ensure that when we downsize
  # back to 3 nodes, we're falling below the quorum size of the larger cluster.
  # If the role successfully downsizes without the cluster going down, we know
  # we've got the cluster management right!
  - <<: *instance_config
    name: vault-4
    groups:
      - vault_all
  
  - <<: *instance_config
    name: vault-5
    groups:
      - vault_all
  
  - <<: *instance_config
    name: vault-6
    groups:
      - vault_all
  
  - <<: *instance_config
    name: vault-7
    groups:
      - vault_all

scenario:
  test_sequence:
    - dependency
    - cleanup
    - destroy
    - syntax
    - create
    - prepare
    
    # NB: In the test sequence below, the reset_down_detector side-effect will
    # start the down-detector (clearing any previous faults). The verifier will
    # check that no outage ocurred.
    #
    # The stop_down_detector side-effect will stop the down detector, clearing
    # any faults. When the down detector is stopped, the verifier will not
    # check for outages.
    
    # Fresh installation
    - converge
    - verify
    - idempotence
    
    # Unseal a server (whilst cluster up)
    - side_effect side_effects/reset_down_detector.yml
    - side_effect side_effects/seal_one_server.yml
    - converge
    - verify
    
    # Unseal servers when cluster down, but some unsealed
    - side_effect side_effects/stop_down_detector.yml
    - side_effect side_effects/seal_quorum_of_servers.yml
    - converge
    - verify
    
    # Unseal whole cluster from cold
    - side_effect side_effects/stop_down_detector.yml
    - side_effect side_effects/seal_all_servers.yml
    - converge
    - verify
    
    # Verify rolling version downgrades/upgrades work
    - side_effect side_effects/reset_down_detector.yml
    # XXX have to use 'verify' rather than 'converge' to allow specifying
    # alternative playbook...
    - verify converge_old_version.yml  # Downgrade
    - converge  # Upgrade back again
    - verify
    
    # Verify rekeying works
    - side_effect side_effects/reset_down_detector.yml
    # XXX have to use 'verify' rather than 'converge' to allow specifying
    # alternative playbook...
    - verify converge_rekey.yml
    - verify
    
    # Verify changing set of administrators and running with only a partial set
    # of unseal keys works
    - side_effect side_effects/reset_down_detector.yml
    # XXX have to use 'verify' rather than 'converge' to allow specifying
    # alternative playbook...
    - verify converge_supply_additional_keys.yml
    - verify
    
    # Verify growing the cluster works
    - side_effect side_effects/reset_down_detector.yml
    # XXX have to use 'verify' rather than 'converge' to allow specifying
    # alternative playbook...
    - verify converge_extended.yml
    - verify verify_extended.yml
    
    # Verify shrinking the cluster works too
    - side_effect side_effects/reset_down_detector.yml
    - converge  # Shrink cluster back down again
    - verify
    
    # Verify backups are usable
    - verify side_effects/test_backups.yml
    - verify  # And that we didn't break anything
    
    - cleanup
    - destroy

provisioner:
  env:
    # Run in an custom ephemeral GnuPG environment
    GNUPGHOME: $MOLECULE_EPHEMERAL_DIRECTORY/gnupghome
  
  config_options:
    connection:
      pipelining: true
  
  inventory:
    group_vars:
      vault_all:
        bbcrd_vault_become: false
        
        bbcrd_vault_version: "1.17.1"
        
        # Older version used in version upgrade/downgrade tests
        bbcrd_vault_rollback_version: "1.17.0"
        
        # No real secrets in the test suite!
        bbcrd_vault_no_log_sensitive: false
        
        # Use HTTP to avoid need to setup certificates in test environment
        bbcrd_vault_listen_protocol: http
        
        bbcrd_vault_public_url: "http://{{ ansible_host | default(inventory_hostname) }}:8200"
        
        bbcrd_vault_unseal_key_threshold: 2
        
        bbcrd_vault_administrators:
          # The default administrator for most tests. Deliberately has one more
          # key than needed to make sure the role can handle that scenario.
          test-user-1:
            bbcrd_vault_unseal_key_shares: 3
          
          # Additional users for tests involving administrators holding
          # incomplete sets of keys.
          test-user-2:
            bbcrd_vault_unseal_key_shares: 1
          test-user-3:
            bbcrd_vault_unseal_key_shares: 1
        
        # Make the token TTL short so that waiting for this to expire won't
        # take too long. This is necessary as a work-around in this test suite
        # for Vault/OpenBao bugs:
        #
        # https://github.com/hashicorp/vault/issues/28378
        # https://github.com/openbao/openbao/issues/522
        #
        # These bugs cause token used to create the backup to live long enough
        # to cause our "no left behind tokens" verify step to fail.
        bbcrd_vault_backup_token_ttl: 20
        
        # Deliberately make the backup timer never fire. This avoids an
        # unluckily timed backup causing a short-lived vault token to exist
        # during the "no tokens left behind" verification step.
        bbcrd_vault_backup_schedule: "2000-1-1 01:00:00"
        
        # Use the test suite's GnuPG environment rather than letting the
        # playbook create its own.
        bbcrd_vault_gnupg_home: "{{ lookup('env', 'GNUPGHOME') }}"

